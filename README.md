## ğŸŒ Chose Your Language / Elige tu idioma:
- [English](#english-gb)
- [EspaÃ±ol](#espaÃ±ol-es)

---

## English GB

# What is JAX?
**JAX** is a new Python library for machine learning developed by Google, designed for high-performance numerical computation, especially in machine learning and optimization. It is an enhanced alternative to NumPy, although its API for numerical functions is based on it.

The advantage of JAX is that it was conceived for both execution modes (Eager and Graph) from the beginning, avoiding the issues of its predecessors _PyTorch_ and _Tensorflow_. 

JAX is widely used in neural network models, such as in libraries built on top of it, like [**Flax**](https://flax.readthedocs.io/en/latest/) and [**Haiku**](https://dm-haiku.readthedocs.io/en/latest/). 

>[!NOTE]
>ğŸ’¡
>JAX might currently be the most advanced in terms of `Machine Learning` (ML) and promises to make machine learning programming more intuitive, structured, and clean. And most importantly, it can replace **Tensorflow** and **PyTorch** with significant advantages.


# Features
1. **Automatic Differentiation**: Efficiently computes derivatives of functions with `jax.grad()`, useful in optimization and deep learning. ğŸ”§
2. **Just-In-Time (JIT) Compilation**: Uses `jax.jit()` to accelerate code by compiling it with **XLA** (Accelerated Linear Algebra). âš¡
3. **GPU/TPU Execution**:  Runs code on CPU, GPU, and TPU without modification. ğŸ’»ğŸ–¥ï¸
4. **Automatic Vectorization**: With `jax.vmap()`, applies operations to multiple data points in parallel. ğŸ”„
5. **NumPy-like Operations**: Provides a very similar API to NumPy (`jax.numpy`), easing the transition. â¡ï¸


# Comparison: JAX vs TensorFlow vs PyTorch

JAX, TensorFlow, and PyTorch are popular libraries for numerical computation and deep learning. This comparison highlights their key differences in performance, ease of use, and application.

## Comparison Table

| **Feature**       | **JAX** ğŸ¦  | **TensorFlow** ğŸ”µ | **PyTorch** ğŸ”¥ |
|-------------------------|------------|----------------|----------------|
| **Paradigm** | Functional, Stateless | Declarative (Graphs) | Imperative (Define-by-Run) |
| **Automatic Differentiation** | âœ… `jax.grad()` | âœ… `tf.GradientTape()` | âœ… `torch.autograd` |
| **JIT Compilation (Just-In-Time)** | âœ… `jax.jit()` (XLA) | âœ… `tf.function()` (XLA) | âŒ (Only in TorchScript) |
| **Runs on GPU/TPU** | âœ… Automatic | âœ… `tf.device()` | âœ… `cuda()` |
| **Automatic Vectorization** | âœ… `jax.vmap()` | âŒ Not native | âŒ Not native |
| **NumPy Usage** | âœ… `jax.numpy` | âŒ Not direct | âœ… Easy conversion |
| **Ecosystem & Pretrained Models** | â›°ï¸ Growing (Flax, Haiku) | ğŸš€ Extensive (TF Hub, Keras) | ğŸ”¥ Vast (Torch Hub) |
| **Learning Curve** | ğŸŸ  Medium | ğŸ”´ High | ğŸŸ¢ Low |

# ğŸ¯ Choosing the Best Option
- **JAX**: Ideal for **numerical optimizations, advanced differentiation, and GPU/TPU computation**. Used in research. ğŸŒ
- **TensorFlow**: Best for **production deployment and scalable applications** en the cloud. â˜ï¸
- **PyTorch**: Perfect for **apid prototyping, research, and ease of use**. âš¡


# ğŸŒEcosystem
## ğŸ“Œ Libraries Built on JAX

### **Deep Learning Frameworks**
- **[Flax](https://github.com/google/flax)** ğŸ—ï¸ â€“ Modular, similar to PyTorch Lightning.
- **[Haiku](https://github.com/deepmind/dm-haiku)** ğŸ”ï¸ â€“ From DeepMind, scope-based structure.
- **[Objax](https://github.com/google/objax)** ğŸ¯ â€“ Object-oriented approach for ML in production.
- **[Equinox](https://github.com/patrick-kidger/equinox)** ğŸŒ± â€“ Functional models, without scopes.

### **Optimization and Mathematics**
- **[Optax](https://github.com/deepmind/optax)** ğŸ› ï¸ â€“ Advanced optimization compatible with Flax and Haiku.
- **[Chex](https://github.com/deepmind/chex)** âœ… â€“ Debugging and testing tools.

### **Reinforcement Learning and Graph Networks**
- **[RLax](https://github.com/deepmind/rlax)** ğŸ† â€“ RL algorithms compatible with JAX.
- **[Jraph](https://github.com/deepmind/jraph)** ğŸ”— â€“ Graph neural networks (GNNs) in JAX.

### **Integrations and Additional Tools**
- **XLA** ğŸš€ â€“ Just-In-Time compilation to accelerate execution on GPU/TPU.
- **`functools.partial()`** ğŸ› ï¸ â€“ Modularity and advanced functionality.

## ğŸ’¡ Which to Choose?
- **For Deep Learning** â†’ Flax or Haiku. ğŸ§ 
- **For Advanced Optimization** â†’ Optax. ğŸ”§
- **For Reinforcement Learning** â†’ RLax. ğŸ…
- **For Graph Neural Networks** â†’ Jraph. ğŸ§¬
- **For Debugging and Testing** â†’ Chex. ğŸ› ï¸


# Example
## Simple Optimization with JAX
#### The code for the example is in the file [`optimizaciÃ³n_simple_con_jax.py`](https://github.com/DavidMoCe/what_is_jax/blob/main/optimizaci%C3%B3n_simple_con_jax.py)
Let's compute the minimum of a simple function, such as the **quadratic function** \( f(x) = x^2 + 3x + 2 \).

1. **Define the function**: We create the function whose derivative we want to compute. ğŸ“
2. **Automatic Differentiation**: We use `jax.grad()` to compute the derivative of the function. ğŸ“
3. **Optimization**: We use the derivative to perform a simple optimization (gradient descent). â¬‡ï¸

## Explanation:
- **Defining the function**:  
  \( f(x) = x^2 + 3x + 2 \) is a simple quadratic function.

- **Calculating the derivative**:  
  `jax.grad(func)` gives us the derivative of the function,  
  \( f'(x) = 2x + 3 \).

- **Optimization**:  
  We use **gradient descent**: we take the derivative value and use it to adjust the value of \( x \) in each iteration to minimize the function.

## Expected Output:

The output will show how the value of \( x \) changes with each iteration until it converges to the minimum of the function. The minimum of the function \( f(x) \) es \( x = -1.5 \).

# Example 2
## Wine Classification with JAX
#### The code for the example is in the file [`jax_calidad_del_vino.py`](https://github.com/DavidMoCe/what_is_jax/blob/main/jax_calidad_del_vino.py)
This project implements a neural network in JAX to classify wine quality using the `wine` dataset from `sklearn`.

## Main Steps:
1. **Data Loading and Preprocessing** ğŸ“Š
   - The dataset is loaded using `sklearn.datasets.load_wine()`.
   - Data is normalized with `StandardScaler` to improve model performance.
   
2. **Parameter Initialization** ğŸ›ï¸
   - We generate random weights and biases using `jax.random.normal`.
   - These are stored in a dictionary for use in the neural network.

3. **Model Definition** ğŸ§ 
   - A neural network with three layers is implemented:
     - Input layer: 16 neurons with ReLU activation.
     - Hidden layer: 8 neurons with ReLU activation.
     - Output layer: 3 neurons with Softmax activation.
   - `jax.numpy.dot` is used for matrix multiplications.

4. **Loss Function and Optimization** ğŸ”§
   - `jax.nn.one_hot` is used to convert labels into vectors.
   - Cross-entropy is calculated to evaluate model error.
   - The `optax.adam` optimizer is used with a learning rate of `0.001`.

5. **Model Training** ğŸš€
   - Weights are updated using `grad(loss_fn)`.
   - The model is trained for 200 epochs.
   - Loss is displayed every 10 epochs.

6. **Model Evaluation** ğŸ“ˆ
   - Accuracy is computed on the test set using `accuracy_score`.
   - A confusion matrix is visualized using `ConfusionMatrixDisplay`.

## Expected Output:
The trained model classifies wines with an accuracy above 90% on the test set.

# References
## Information on JAX
- [https://eiposgrados.com/blog-python/jax-machine-learning/](https://eiposgrados.com/blog-python/jax-machine-learning/)
- [https://es.eitca.org/inteligencia-artificial/eitc-ai-gcml-google-nube-aprendizaje-autom%C3%A1tico/plataforma-google-cloud-ai/introducci%C3%B3n-a-jax/revisi%C3%B3n-de-examen-introducci%C3%B3n-a-jax/%C2%BFCu%C3%A1les-son-las-caracter%C3%ADsticas-de-jax-que-permiten-el-m%C3%A1ximo-rendimiento-en-el-entorno-de-python%3F/](https://es.eitca.org/inteligencia-artificial/eitc-ai-gcml-google-nube-aprendizaje-autom%C3%A1tico/plataforma-google-cloud-ai/introducci%C3%B3n-a-jax/revisi%C3%B3n-de-examen-introducci%C3%B3n-a-jax/%C2%BFCu%C3%A1les-son-las-caracter%C3%ADsticas-de-jax-que-permiten-el-m%C3%A1ximo-rendimiento-en-el-entorno-de-python%3F/)

## TensorFlow and PyTorch
- [https://www.tensorflow.org/?hl=es-419](https://www.tensorflow.org/?hl=es-419)
- [https://pytorch.org/](https://pytorch.org/)

## Flax and Haiku
- [https://flax.readthedocs.io/en/latest/](https://flax.readthedocs.io/en/latest/)
- [https://dm-haiku.readthedocs.io/en/latest/](https://dm-haiku.readthedocs.io/en/latest/)

## Comparison between JAX, TensorFlow, and PyTorch
- [https://www.computerworld.es/article/2115282/tensorflow-pytorch-y-jax-los-principales-marcos-de-deep-learning.html](https://www.computerworld.es/article/2115282/tensorflow-pytorch-y-jax-los-principales-marcos-de-deep-learning.html)

## Collecting and Structuring Information
- [https://chatgpt.com/](https://chatgpt.com/)

# ğŸ“ƒ License
This project is under the MIT License. See the [`LICENSE`](https://github.com/DavidMoCe/what_is_jax/blob/main/LICENSE.TXT) file for more details.

---

## EspaÃ±ol ES

# Â¿QuÃ© es JAX?
**JAX** es una nueva biblioteca de Python de aprendizaje automÃ¡tico de Google, diseÃ±ada para la computaciÃ³n numÃ©rica de alto rendimiento, especialmente en aprendizaje automÃ¡tico y optimizaciÃ³n. Es una alternativa mejorada a NumPy aunque su API para funciones numÃ©ricas se basa en esta.

La ventaja de JAX es que fue concebido para ambos modos de ejecuciÃ³n (Eager y Graph) desde el principio y adolece de los problemas de sus predecesores _PyTorch_ y _Tensorflow_. 

JAX es ampliamente utilizado en modelos de redes neuronales, como en bibliotecas construidas sobre Ã©l, como [**Flax**](https://flax.readthedocs.io/en/latest/) y [**Haiku**](https://dm-haiku.readthedocs.io/en/latest/). 

>[!NOTE]
>ğŸ’¡
>JAX quizÃ¡s sea actualmente lo mÃ¡s avanzado en tÃ©rminos `Machine Learning` (ML) y promete hacer que la programaciÃ³n de aprendizaje automÃ¡tico sea mÃ¡s intuitiva, estructurada y limpia. Y, sobre todo, puede reemplazar con importantes ventajas a **Tensorflow** y **PyTorch**.


# CaracterÃ­sticas
1. **DiferenciaciÃ³n automÃ¡tica**: Permite calcular derivadas de funciones de manera eficiente con `jax.grad()`, Ãºtil en optimizaciÃ³n y aprendizaje profundo. ğŸ”§
2. **CompilaciÃ³n Just-In-Time (JIT)**: Usa `jax.jit()` para acelerar el cÃ³digo al compilarlo con **XLA** (Accelerated Linear Algebra). âš¡
3. **EjecuciÃ³n en GPU/TPU**: Puede ejecutar cÃ³digo en CPU, GPU y TPU sin necesidad de cambios. ğŸ’»ğŸ–¥ï¸
4. **VectorizaciÃ³n automÃ¡tica**: Con `jax.vmap()`, permite aplicar operaciones a mÃºltiples datos en paralelo. ğŸ”„
5. **Operaciones similares a NumPy**: Proporciona una API muy parecida a NumPy (`jax.numpy`), facilitando la transiciÃ³n. â¡ï¸


# ComparaciÃ³n: JAX vs TensorFlow vs PyTorch

JAX, TensorFlow y PyTorch son bibliotecas populares para cÃ³mputo numÃ©rico y aprendizaje profundo. Esta comparaciÃ³n destaca sus diferencias clave en rendimiento, facilidad de uso y aplicaciÃ³n.

## Tabla Comparativa

| **CaracterÃ­stica**       | **JAX** ğŸ¦  | **TensorFlow** ğŸ”µ | **PyTorch** ğŸ”¥ |
|-------------------------|------------|----------------|----------------|
| **Paradigma** | Funcional, sin estado | Declarativo (Graphs) | Imperativo (Define-by-Run) |
| **DiferenciaciÃ³n AutomÃ¡tica** | âœ… `jax.grad()` | âœ… `tf.GradientTape()` | âœ… `torch.autograd` |
| **CompilaciÃ³n JIT (Just-In-Time)** | âœ… `jax.jit()` (XLA) | âœ… `tf.function()` (XLA) | âŒ (Solo en TorchScript) |
| **Ejecuta en GPU/TPU** | âœ… AutomÃ¡tico | âœ… `tf.device()` | âœ… `cuda()` |
| **VectorizaciÃ³n AutomÃ¡tica** | âœ… `jax.vmap()` | âŒ No nativo | âŒ No nativo |
| **Uso de NumPy** | âœ… `jax.numpy` | âŒ No directo | âœ… ConversiÃ³n fÃ¡cil |
| **Ecosistema y Modelos Preentrenados** | â›°ï¸ Creciendo (Flax, Haiku) | ğŸš€ Amplio (TF Hub, Keras) | ğŸ”¥ Extenso (Torch Hub) |
| **Curva de Aprendizaje** | ğŸŸ  Media | ğŸ”´ Alta | ğŸŸ¢ Baja |

# ğŸ¯ ElecciÃ³n de la Mejor OpciÃ³n
- **JAX**: Ideal para **optimizaciones numÃ©ricas, diferenciaciÃ³n avanzada y computaciÃ³n en GPU/TPU**. Usado en investigaciÃ³n. ğŸŒ
- **TensorFlow**: Mejor para **despliegue en producciÃ³n y aplicaciones escalables** en la nube. â˜ï¸
- **PyTorch**: Perfecto para **prototipado rÃ¡pido, investigaciÃ³n y facilidad de uso**. âš¡


# ğŸŒEcosistema
## ğŸ“Œ LibrerÃ­as Implementadas sobre JAX

### **Frameworks de Deep Learning**
- **[Flax](https://github.com/google/flax)** ğŸ—ï¸ â€“ Modular, similar a PyTorch Lightning.
- **[Haiku](https://github.com/deepmind/dm-haiku)** ğŸ”ï¸ â€“ De DeepMind, estructura basada en scopes.
- **[Objax](https://github.com/google/objax)** ğŸ¯ â€“ Enfoque orientado a objetos para ML en producciÃ³n.
- **[Equinox](https://github.com/patrick-kidger/equinox)** ğŸŒ± â€“ Modelos funcionales, sin scopes.

### **OptimizaciÃ³n y MatemÃ¡ticas**
- **[Optax](https://github.com/deepmind/optax)** ğŸ› ï¸ â€“ OptimizaciÃ³n avanzada compatible con Flax y Haiku.
- **[Chex](https://github.com/deepmind/chex)** âœ… â€“ Herramientas de depuraciÃ³n y testing.

### **Aprendizaje por Refuerzo y Redes GrÃ¡ficas**
- **[RLax](https://github.com/deepmind/rlax)** ğŸ† â€“ Algoritmos de RL compatibles con JAX.
- **[Jraph](https://github.com/deepmind/jraph)** ğŸ”— â€“ Redes neuronales grÃ¡ficas (GNNs) en JAX.

### **Integraciones y Herramientas Adicionales**
- **XLA** ğŸš€ â€“ CompilaciÃ³n Just-In-Time para acelerar ejecuciÃ³n en GPU/TPU.
- **`functools.partial()`** ğŸ› ï¸ â€“ Modularidad y funcionalidad avanzada.

## ğŸ’¡ Â¿CuÃ¡l Elegir?
- **Para Deep Learning** â†’ Flax o Haiku. ğŸ§ 
- **Para OptimizaciÃ³n Avanzada** â†’ Optax. ğŸ”§
- **Para Aprendizaje por Refuerzo** â†’ RLax. ğŸ…
- **Para Redes Neuronales GrÃ¡ficas** â†’ Jraph. ğŸ§¬
- **Para DepuraciÃ³n y Pruebas** â†’ Chex. ğŸ› ï¸


# Ejemplo
## OptimizaciÃ³n simple con JAX
#### El cÃ³digo del ejemplo esta en el archivo [`optimizaciÃ³n_simple_con_jax.py`](https://github.com/DavidMoCe/what_is_jax/blob/main/optimizaci%C3%B3n_simple_con_jax.py)
Vamos a calcular el mÃ­nimo de una funciÃ³n simple, como la **funciÃ³n cuadrÃ¡tica** \( f(x) = x^2 + 3x + 2 \).

1. **Definir la funciÃ³n**: Creamos la funciÃ³n cuya derivada queremos calcular. ğŸ“
2. **DiferenciaciÃ³n AutomÃ¡tica**: Usamos `jax.grad()` para calcular la derivada de la funciÃ³n. ğŸ“
3. **OptimizaciÃ³n**: Usamos la derivada para hacer una optimizaciÃ³n simple (descenso por gradiente). â¬‡ï¸

## ExplicaciÃ³n:
- **DefiniciÃ³n de la funciÃ³n**:  
  \( f(x) = x^2 + 3x + 2 \) es una funciÃ³n cuadrÃ¡tica simple.

- **CÃ¡lculo de la derivada**:  
  `jax.grad(func)` nos da la derivada de la funciÃ³n, es decir,  
  \( f'(x) = 2x + 3 \).

- **OptimizaciÃ³n**:  
  Usamos **descenso por gradiente**: tomamos el valor de la derivada y lo usamos para ajustar el valor de \( x \) en cada iteraciÃ³n, con el fin de minimizar la funciÃ³n.

## Resultado esperado:

La salida te mostrarÃ¡ cÃ³mo el valor de \( x \) cambia en cada iteraciÃ³n hasta converger al mÃ­nimo de la funciÃ³n. El mÃ­nimo de la funciÃ³n \( f(x) \) es \( x = -1.5 \).

# Ejemplo 2
## ClasificaciÃ³n de Vinos con JAX
#### El cÃ³digo del ejemplo esta en el archivo [`jax_calidad_del_vino.py`](https://github.com/DavidMoCe/what_is_jax/blob/main/jax_calidad_del_vino.py)
Este proyecto implementa una red neuronal en JAX para clasificar la calidad del vino utilizando el conjunto de datos `wine` de `sklearn`.

## Pasos que realiza el proyecto:
1. Carga y preprocesamiento de datos ğŸ“Š
    - Se carga el dataset de `sklearn.datasets.load_wine()`.
    - Se normalizan los datos con `StandardScaler` para mejorar el rendimiento del modelo.

2. InicializaciÃ³n de parÃ¡metros ğŸ›ï¸
    - Se generan pesos y sesgos aleatorios usando `jax.random.normal`.
    - Se almacenan en un diccionario para su uso en la red neuronal.

3. DefiniciÃ³n del modelo ğŸ§ 
    - Se implementa una red neuronal con tres capas:
       - Capa de entrada: 16 neuronas y activaciÃ³n ReLU.
       - Capa oculta: 8 neuronas y activaciÃ³n ReLU.
       - Capa de salida: 3 neuronas con activaciÃ³n Softmax.
    - Se usa `jax.numpy.dot` para las multiplicaciones de matrices.

4. FunciÃ³n de pÃ©rdida y optimizaciÃ³n ğŸ”§
    - Se usa `jax.nn.one_hot` para convertir etiquetas en vectores.
    - Se calcula la entropÃ­a cruzada para evaluar el error del modelo.
    - Se usa el optimizador `optax.adam` con una tasa de aprendizaje de `0.001`.

5. Entrenamiento del modelo ğŸš€
    - Se actualizan los pesos usando `grad(loss_fn)`.
    - Se entrena durante 200 Ã©pocas.
    - Se muestra la pÃ©rdida cada 10 Ã©pocas.

6. EvaluaciÃ³n del modelo ğŸ“ˆ
    - Se calcula la precisiÃ³n en el conjunto de prueba con `accuracy_score`.
    - Se visualiza la matriz de confusiÃ³n con `ConfusionMatrixDisplay`.

## Resultado esperado:
El modelo entrenado clasifica los vinos con una precisiÃ³n superior al 90% en el conjunto de prueba.

# BibliografÃ­a
## InformaciÃ³n sobre JAX
- [https://eiposgrados.com/blog-python/jax-machine-learning/](https://eiposgrados.com/blog-python/jax-machine-learning/)
- [https://es.eitca.org/inteligencia-artificial/eitc-ai-gcml-google-nube-aprendizaje-autom%C3%A1tico/plataforma-google-cloud-ai/introducci%C3%B3n-a-jax/revisi%C3%B3n-de-examen-introducci%C3%B3n-a-jax/%C2%BFCu%C3%A1les-son-las-caracter%C3%ADsticas-de-jax-que-permiten-el-m%C3%A1ximo-rendimiento-en-el-entorno-de-python%3F/](https://es.eitca.org/inteligencia-artificial/eitc-ai-gcml-google-nube-aprendizaje-autom%C3%A1tico/plataforma-google-cloud-ai/introducci%C3%B3n-a-jax/revisi%C3%B3n-de-examen-introducci%C3%B3n-a-jax/%C2%BFCu%C3%A1les-son-las-caracter%C3%ADsticas-de-jax-que-permiten-el-m%C3%A1ximo-rendimiento-en-el-entorno-de-python%3F/)

## TensorFlow y PyTorch
- [https://www.tensorflow.org/?hl=es-419](https://www.tensorflow.org/?hl=es-419)
- [https://pytorch.org/](https://pytorch.org/)

## Flax y Haiku
- [https://flax.readthedocs.io/en/latest/](https://flax.readthedocs.io/en/latest/)
- [https://dm-haiku.readthedocs.io/en/latest/](https://dm-haiku.readthedocs.io/en/latest/)

## Comparativa entre JAX, TensorFlow y PyTorch
- [https://www.computerworld.es/article/2115282/tensorflow-pytorch-y-jax-los-principales-marcos-de-deep-learning.html](https://www.computerworld.es/article/2115282/tensorflow-pytorch-y-jax-los-principales-marcos-de-deep-learning.html)

## Recopilar y estructurar la informaciÃ³n
- [https://chatgpt.com/](https://chatgpt.com/)

# ğŸ“ƒ Licencia
Este proyecto estÃ¡ bajo la licencia MIT. Consulta el archivo [`LICENSE`](https://github.com/DavidMoCe/what_is_jax/blob/main/LICENSE.TXT) para mÃ¡s detalles.

