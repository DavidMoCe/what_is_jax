## üåç Chose Your Language / Elige tu idioma:
- [English](#english-gb)
- [Espa√±ol](#espa√±ol-es)

---

## English GB

# What is JAX?
**JAX** is a new Python library for machine learning developed by Google, designed for high-performance numerical computation, especially in machine learning and optimization. It is an enhanced alternative to NumPy, although its API for numerical functions is based on it.

The advantage of JAX is that it was conceived for both execution modes (Eager and Graph) from the beginning, avoiding the issues of its predecessors _PyTorch_ and _Tensorflow_. 

JAX is widely used in neural network models, such as in libraries built on top of it, like [**Flax**](https://flax.readthedocs.io/en/latest/) and [**Haiku**](https://dm-haiku.readthedocs.io/en/latest/). 

>[!NOTE]
>üí°
>JAX might currently be the most advanced in terms of `Machine Learning` (ML) and promises to make machine learning programming more intuitive, structured, and clean. And most importantly, it can replace **Tensorflow** and **PyTorch** with significant advantages.


# Features
1. **Automatic Differentiation**: Efficiently computes derivatives of functions with `jax.grad()`, useful in optimization and deep learning. üîß
2. **Just-In-Time (JIT) Compilation**: Uses `jax.jit()` to accelerate code by compiling it with **XLA** (Accelerated Linear Algebra). ‚ö°
3. **GPU/TPU Execution**:  Runs code on CPU, GPU, and TPU without modification. üíªüñ•Ô∏è
4. **Automatic Vectorization**: With `jax.vmap()`, applies operations to multiple data points in parallel. üîÑ
5. **NumPy-like Operations**: Provides a very similar API to NumPy (`jax.numpy`), easing the transition. ‚û°Ô∏è


# Comparison: JAX vs TensorFlow vs PyTorch

JAX, TensorFlow, and PyTorch are popular libraries for numerical computation and deep learning. This comparison highlights their key differences in performance, ease of use, and application.

## Comparison Table

| **Feature**       | **JAX** ü¶é  | **TensorFlow** üîµ | **PyTorch** üî• |
|-------------------------|------------|----------------|----------------|
| **Paradigm** | Functional, Stateless | Declarative (Graphs) | Imperative (Define-by-Run) |
| **Automatic Differentiation** | ‚úÖ `jax.grad()` | ‚úÖ `tf.GradientTape()` | ‚úÖ `torch.autograd` |
| **JIT Compilation (Just-In-Time)** | ‚úÖ `jax.jit()` (XLA) | ‚úÖ `tf.function()` (XLA) | ‚ùå (Only in TorchScript) |
| **Runs on GPU/TPU** | ‚úÖ Automatic | ‚úÖ `tf.device()` | ‚úÖ `cuda()` |
| **Automatic Vectorization** | ‚úÖ `jax.vmap()` | ‚ùå Not native | ‚ùå Not native |
| **NumPy Usage** | ‚úÖ `jax.numpy` | ‚ùå Not direct | ‚úÖ Easy conversion |
| **Ecosystem & Pretrained Models** | ‚õ∞Ô∏è Growing (Flax, Haiku) | üöÄ Extensive (TF Hub, Keras) | üî• Vast (Torch Hub) |
| **Learning Curve** | üü† Medium | üî¥ High | üü¢ Low |

# üéØ Choosing the Best Option
- **JAX**: Ideal for **numerical optimizations, advanced differentiation, and GPU/TPU computation**. Used in research. üåê
- **TensorFlow**: Best for **production deployment and scalable applications** en the cloud. ‚òÅÔ∏è
- **PyTorch**: Perfect for **apid prototyping, research, and ease of use**. ‚ö°


# üåçEcosystem
## üìå Libraries Built on JAX

### **Deep Learning Frameworks**
- **[Flax](https://github.com/google/flax)** üèóÔ∏è ‚Äì Modular, similar to PyTorch Lightning.
- **[Haiku](https://github.com/deepmind/dm-haiku)** üèîÔ∏è ‚Äì From DeepMind, scope-based structure.
- **[Objax](https://github.com/google/objax)** üéØ ‚Äì Object-oriented approach for ML in production.
- **[Equinox](https://github.com/patrick-kidger/equinox)** üå± ‚Äì Functional models, without scopes.

### **Optimization and Mathematics**
- **[Optax](https://github.com/deepmind/optax)** üõ†Ô∏è ‚Äì Advanced optimization compatible with Flax and Haiku.
- **[Chex](https://github.com/deepmind/chex)** ‚úÖ ‚Äì Debugging and testing tools.

### **Reinforcement Learning and Graph Networks**
- **[RLax](https://github.com/deepmind/rlax)** üèÜ ‚Äì RL algorithms compatible with JAX.
- **[Jraph](https://github.com/deepmind/jraph)** üîó ‚Äì Graph neural networks (GNNs) in JAX.

### **Integrations and Additional Tools**
- **XLA** üöÄ ‚Äì Just-In-Time compilation to accelerate execution on GPU/TPU.
- **`functools.partial()`** üõ†Ô∏è ‚Äì Modularity and advanced functionality.

## üí° Which to Choose?
- **For Deep Learning** ‚Üí Flax or Haiku. üß†
- **For Advanced Optimization** ‚Üí Optax. üîß
- **For Reinforcement Learning** ‚Üí RLax. üèÖ
- **For Graph Neural Networks** ‚Üí Jraph. üß¨
- **For Debugging and Testing** ‚Üí Chex. üõ†Ô∏è


# Example
## Simple Optimization with JAX
#### The code for the example is in the file [`optimizaci√≥n_simple_con_jax.py`](https://github.com/DavidMoCe/what_is_jax/blob/main/optimizaci%C3%B3n_simple_con_jax.py)
Let's compute the minimum of a simple function, such as the **quadratic function** \( f(x) = x^2 + 3x + 2 \).

1. **Define the function**: We create the function whose derivative we want to compute. üìù
2. **Automatic Differentiation**: We use `jax.grad()` to compute the derivative of the function. üìê
3. **Optimization**: We use the derivative to perform a simple optimization (gradient descent). ‚¨áÔ∏è

## Explanation:
- **Defining the function**:  
  \( f(x) = x^2 + 3x + 2 \) is a simple quadratic function.

- **Calculating the derivative**:  
  `jax.grad(func)` gives us the derivative of the function,  
  \( f'(x) = 2x + 3 \).

- **Optimization**:  
  We use **gradient descent**: we take the derivative value and use it to adjust the value of \( x \) in each iteration to minimize the function.

## Expected Output:

The output will show how the value of \( x \) changes with each iteration until it converges to the minimum of the function. The minimum of the function \( f(x) \) es \( x = -1.5 \).

# Example 2
## Wine Classification with JAX
#### The code for the example is in the file [`jax_calidad_del_vino.py`](https://github.com/DavidMoCe/what_is_jax/blob/main/jax_calidad_del_vino.py)
This project implements a neural network in JAX to classify wine quality using the `wine` dataset from `sklearn`.

## Main Steps:
1. **Data Loading and Preprocessing** üìä
   - The dataset is loaded using `sklearn.datasets.load_wine()`.
   - Data is normalized with `StandardScaler` to improve model performance.
   
2. **Parameter Initialization** üéõÔ∏è
   - We generate random weights and biases using `jax.random.normal`.
   - These are stored in a dictionary for use in the neural network.

3. **Model Definition** üß†
   - A neural network with three layers is implemented:
     - Input layer: 16 neurons with ReLU activation.
     - Hidden layer: 8 neurons with ReLU activation.
     - Output layer: 3 neurons with Softmax activation.
   - `jax.numpy.dot` is used for matrix multiplications.

4. **Loss Function and Optimization** üîß
   - `jax.nn.one_hot` is used to convert labels into vectors.
   - Cross-entropy is calculated to evaluate model error.
   - The `optax.adam` optimizer is used with a learning rate of `0.001`.

5. **Model Training** üöÄ
   - Weights are updated using `grad(loss_fn)`.
   - The model is trained for 200 epochs.
   - Loss is displayed every 10 epochs.

6. **Model Evaluation** üìà
   - Accuracy is computed on the test set using `accuracy_score`.
   - A confusion matrix is visualized using `ConfusionMatrixDisplay`.

## Expected Output:
The trained model classifies wines with an accuracy above 90% on the test set.

# References
## Information on JAX
- [https://eiposgrados.com/blog-python/jax-machine-learning/](https://eiposgrados.com/blog-python/jax-machine-learning/)
- [https://es.eitca.org/inteligencia-artificial/eitc-ai-gcml-google-nube-aprendizaje-autom%C3%A1tico/plataforma-google-cloud-ai/introducci%C3%B3n-a-jax/revisi%C3%B3n-de-examen-introducci%C3%B3n-a-jax/%C2%BFCu%C3%A1les-son-las-caracter%C3%ADsticas-de-jax-que-permiten-el-m%C3%A1ximo-rendimiento-en-el-entorno-de-python%3F/](https://es.eitca.org/inteligencia-artificial/eitc-ai-gcml-google-nube-aprendizaje-autom%C3%A1tico/plataforma-google-cloud-ai/introducci%C3%B3n-a-jax/revisi%C3%B3n-de-examen-introducci%C3%B3n-a-jax/%C2%BFCu%C3%A1les-son-las-caracter%C3%ADsticas-de-jax-que-permiten-el-m%C3%A1ximo-rendimiento-en-el-entorno-de-python%3F/)

## TensorFlow and PyTorch
- [https://www.tensorflow.org/?hl=es-419](https://www.tensorflow.org/?hl=es-419)
- [https://pytorch.org/](https://pytorch.org/)

## Flax and Haiku
- [https://flax.readthedocs.io/en/latest/](https://flax.readthedocs.io/en/latest/)
- [https://dm-haiku.readthedocs.io/en/latest/](https://dm-haiku.readthedocs.io/en/latest/)

## Comparison between JAX, TensorFlow, and PyTorch
- [https://www.computerworld.es/article/2115282/tensorflow-pytorch-y-jax-los-principales-marcos-de-deep-learning.html](https://www.computerworld.es/article/2115282/tensorflow-pytorch-y-jax-los-principales-marcos-de-deep-learning.html)

## Collecting and Structuring Information
- [https://chatgpt.com/](https://chatgpt.com/)

# üìÉ License
This project is under the MIT License. See the [`LICENSE`](https://github.com/DavidMoCe/what_is_jax/blob/main/LICENSE.TXT) file for more details.

---

## Espa√±ol ES

# ¬øQu√© es JAX?
**JAX** es una nueva biblioteca de Python de aprendizaje autom√°tico de Google, dise√±ada para la computaci√≥n num√©rica de alto rendimiento, especialmente en aprendizaje autom√°tico y optimizaci√≥n. Es una alternativa mejorada a NumPy aunque su API para funciones num√©ricas se basa en esta.

La ventaja de JAX es que fue concebido para ambos modos de ejecuci√≥n (Eager y Graph) desde el principio y adolece de los problemas de sus predecesores _PyTorch_ y _Tensorflow_. 

JAX es ampliamente utilizado en modelos de redes neuronales, como en bibliotecas construidas sobre √©l, como [**Flax**](https://flax.readthedocs.io/en/latest/) y [**Haiku**](https://dm-haiku.readthedocs.io/en/latest/). 

>[!NOTE]
>üí°
>JAX quiz√°s sea actualmente lo m√°s avanzado en t√©rminos `Machine Learning` (ML) y promete hacer que la programaci√≥n de aprendizaje autom√°tico sea m√°s intuitiva, estructurada y limpia. Y, sobre todo, puede reemplazar con importantes ventajas a **Tensorflow** y **PyTorch**.


# Caracter√≠sticas
1. **Diferenciaci√≥n autom√°tica**: Permite calcular derivadas de funciones de manera eficiente con `jax.grad()`, √∫til en optimizaci√≥n y aprendizaje profundo. üîß
2. **Compilaci√≥n Just-In-Time (JIT)**: Usa `jax.jit()` para acelerar el c√≥digo al compilarlo con **XLA** (Accelerated Linear Algebra). ‚ö°
3. **Ejecuci√≥n en GPU/TPU**: Puede ejecutar c√≥digo en CPU, GPU y TPU sin necesidad de cambios. üíªüñ•Ô∏è
4. **Vectorizaci√≥n autom√°tica**: Con `jax.vmap()`, permite aplicar operaciones a m√∫ltiples datos en paralelo. üîÑ
5. **Operaciones similares a NumPy**: Proporciona una API muy parecida a NumPy (`jax.numpy`), facilitando la transici√≥n. ‚û°Ô∏è


# Comparaci√≥n: JAX vs TensorFlow vs PyTorch

JAX, TensorFlow y PyTorch son bibliotecas populares para c√≥mputo num√©rico y aprendizaje profundo. Esta comparaci√≥n destaca sus diferencias clave en rendimiento, facilidad de uso y aplicaci√≥n.

## Tabla Comparativa

| **Caracter√≠stica**       | **JAX** ü¶é  | **TensorFlow** üîµ | **PyTorch** üî• |
|-------------------------|------------|----------------|----------------|
| **Paradigma** | Funcional, sin estado | Declarativo (Graphs) | Imperativo (Define-by-Run) |
| **Diferenciaci√≥n Autom√°tica** | ‚úÖ `jax.grad()` | ‚úÖ `tf.GradientTape()` | ‚úÖ `torch.autograd` |
| **Compilaci√≥n JIT (Just-In-Time)** | ‚úÖ `jax.jit()` (XLA) | ‚úÖ `tf.function()` (XLA) | ‚ùå (Solo en TorchScript) |
| **Ejecuta en GPU/TPU** | ‚úÖ Autom√°tico | ‚úÖ `tf.device()` | ‚úÖ `cuda()` |
| **Vectorizaci√≥n Autom√°tica** | ‚úÖ `jax.vmap()` | ‚ùå No nativo | ‚ùå No nativo |
| **Uso de NumPy** | ‚úÖ `jax.numpy` | ‚ùå No directo | ‚úÖ Conversi√≥n f√°cil |
| **Ecosistema y Modelos Preentrenados** | ‚õ∞Ô∏è Creciendo (Flax, Haiku) | üöÄ Amplio (TF Hub, Keras) | üî• Extenso (Torch Hub) |
| **Curva de Aprendizaje** | üü† Media | üî¥ Alta | üü¢ Baja |

# üéØ Elecci√≥n de la Mejor Opci√≥n
- **JAX**: Ideal para **optimizaciones num√©ricas, diferenciaci√≥n avanzada y computaci√≥n en GPU/TPU**. Usado en investigaci√≥n. üåê
- **TensorFlow**: Mejor para **despliegue en producci√≥n y aplicaciones escalables** en la nube. ‚òÅÔ∏è
- **PyTorch**: Perfecto para **prototipado r√°pido, investigaci√≥n y facilidad de uso**. ‚ö°


# üåçEcosistema
## üìå Librer√≠as Implementadas sobre JAX

### **Frameworks de Deep Learning**
- **[Flax](https://github.com/google/flax)** üèóÔ∏è ‚Äì Modular, similar a PyTorch Lightning.
- **[Haiku](https://github.com/deepmind/dm-haiku)** üèîÔ∏è ‚Äì De DeepMind, estructura basada en scopes.
- **[Objax](https://github.com/google/objax)** üéØ ‚Äì Enfoque orientado a objetos para ML en producci√≥n.
- **[Equinox](https://github.com/patrick-kidger/equinox)** üå± ‚Äì Modelos funcionales, sin scopes.

### **Optimizaci√≥n y Matem√°ticas**
- **[Optax](https://github.com/deepmind/optax)** üõ†Ô∏è ‚Äì Optimizaci√≥n avanzada compatible con Flax y Haiku.
- **[Chex](https://github.com/deepmind/chex)** ‚úÖ ‚Äì Herramientas de depuraci√≥n y testing.

### **Aprendizaje por Refuerzo y Redes Gr√°ficas**
- **[RLax](https://github.com/deepmind/rlax)** üèÜ ‚Äì Algoritmos de RL compatibles con JAX.
- **[Jraph](https://github.com/deepmind/jraph)** üîó ‚Äì Redes neuronales gr√°ficas (GNNs) en JAX.

### **Integraciones y Herramientas Adicionales**
- **XLA** üöÄ ‚Äì Compilaci√≥n Just-In-Time para acelerar ejecuci√≥n en GPU/TPU.
- **`functools.partial()`** üõ†Ô∏è ‚Äì Modularidad y funcionalidad avanzada.

## üí° ¬øCu√°l Elegir?
- **Para Deep Learning** ‚Üí Flax o Haiku. üß†
- **Para Optimizaci√≥n Avanzada** ‚Üí Optax. üîß
- **Para Aprendizaje por Refuerzo** ‚Üí RLax. üèÖ
- **Para Redes Neuronales Gr√°ficas** ‚Üí Jraph. üß¨
- **Para Depuraci√≥n y Pruebas** ‚Üí Chex. üõ†Ô∏è


# Ejemplo
## Optimizaci√≥n simple con JAX
#### El c√≥digo del ejemplo esta en el archivo [`optimizaci√≥n_simple_con_jax.py`](https://github.com/DavidMoCe/what_is_jax/blob/main/optimizaci%C3%B3n_simple_con_jax.py)
Vamos a calcular el m√≠nimo de una funci√≥n simple, como la **funci√≥n cuadr√°tica** \( f(x) = x^2 + 3x + 2 \).

1. **Definir la funci√≥n**: Creamos la funci√≥n cuya derivada queremos calcular. üìù
2. **Diferenciaci√≥n Autom√°tica**: Usamos `jax.grad()` para calcular la derivada de la funci√≥n. üìê
3. **Optimizaci√≥n**: Usamos la derivada para hacer una optimizaci√≥n simple (descenso por gradiente). ‚¨áÔ∏è

## Explicaci√≥n:
- **Definici√≥n de la funci√≥n**:  
  \( f(x) = x^2 + 3x + 2 \) es una funci√≥n cuadr√°tica simple.

- **C√°lculo de la derivada**:  
  `jax.grad(func)` nos da la derivada de la funci√≥n, es decir,  
  \( f'(x) = 2x + 3 \).

- **Optimizaci√≥n**:  
  Usamos **descenso por gradiente**: tomamos el valor de la derivada y lo usamos para ajustar el valor de \( x \) en cada iteraci√≥n, con el fin de minimizar la funci√≥n.

## Resultado esperado:

La salida te mostrar√° c√≥mo el valor de \( x \) cambia en cada iteraci√≥n hasta converger al m√≠nimo de la funci√≥n. El m√≠nimo de la funci√≥n \( f(x) \) es \( x = -1.5 \).

# Ejemplo 2
## Clasificaci√≥n de Vinos con JAX
#### El c√≥digo del ejemplo esta en el archivo [`jax_calidad_del_vino.py`](https://github.com/DavidMoCe/what_is_jax/blob/main/jax_calidad_del_vino.py)
Este proyecto implementa una red neuronal en JAX para clasificar la calidad del vino utilizando el conjunto de datos `wine` de `sklearn`.

## Pasos que realiza el proyecto:
1. Carga y preprocesamiento de datos üìä
    - Se carga el dataset de `sklearn.datasets.load_wine()`.
    - Se normalizan los datos con `StandardScaler` para mejorar el rendimiento del modelo.

2. Inicializaci√≥n de par√°metros üéõÔ∏è
    - Se generan pesos y sesgos aleatorios usando `jax.random.normal`.
    - Se almacenan en un diccionario para su uso en la red neuronal.

3. Definici√≥n del modelo üß†
    - Se implementa una red neuronal con tres capas:
       - Capa de entrada: 16 neuronas y activaci√≥n ReLU.
       - Capa oculta: 8 neuronas y activaci√≥n ReLU.
       - Capa de salida: 3 neuronas con activaci√≥n Softmax.
    - Se usa `jax.numpy.dot` para las multiplicaciones de matrices.

4. Funci√≥n de p√©rdida y optimizaci√≥n üîß
    - Se usa `jax.nn.one_hot` para convertir etiquetas en vectores.
    - Se calcula la entrop√≠a cruzada para evaluar el error del modelo.
    - Se usa el optimizador `optax.adam` con una tasa de aprendizaje de `0.001`.

5. Entrenamiento del modelo üöÄ
    - Se actualizan los pesos usando `grad(loss_fn)`.
    - Se entrena durante 200 √©pocas.
    - Se muestra la p√©rdida cada 10 √©pocas.

6. Evaluaci√≥n del modelo üìà
    - Se calcula la precisi√≥n en el conjunto de prueba con `accuracy_score`.
    - Se visualiza la matriz de confusi√≥n con `ConfusionMatrixDisplay`.

## Resultado esperado:
El modelo entrenado clasifica los vinos con una precisi√≥n superior al 90% en el conjunto de prueba.

# Bibliograf√≠a
## Informaci√≥n sobre JAX
- [https://eiposgrados.com/blog-python/jax-machine-learning/](https://eiposgrados.com/blog-python/jax-machine-learning/)
- [https://es.eitca.org/inteligencia-artificial/eitc-ai-gcml-google-nube-aprendizaje-autom%C3%A1tico/plataforma-google-cloud-ai/introducci%C3%B3n-a-jax/revisi%C3%B3n-de-examen-introducci%C3%B3n-a-jax/%C2%BFCu%C3%A1les-son-las-caracter%C3%ADsticas-de-jax-que-permiten-el-m%C3%A1ximo-rendimiento-en-el-entorno-de-python%3F/](https://es.eitca.org/inteligencia-artificial/eitc-ai-gcml-google-nube-aprendizaje-autom%C3%A1tico/plataforma-google-cloud-ai/introducci%C3%B3n-a-jax/revisi%C3%B3n-de-examen-introducci%C3%B3n-a-jax/%C2%BFCu%C3%A1les-son-las-caracter%C3%ADsticas-de-jax-que-permiten-el-m%C3%A1ximo-rendimiento-en-el-entorno-de-python%3F/)

## TensorFlow y PyTorch
- [https://www.tensorflow.org/?hl=es-419](https://www.tensorflow.org/?hl=es-419)
- [https://pytorch.org/](https://pytorch.org/)

## Flax y Haiku
- [https://flax.readthedocs.io/en/latest/](https://flax.readthedocs.io/en/latest/)
- [https://dm-haiku.readthedocs.io/en/latest/](https://dm-haiku.readthedocs.io/en/latest/)

## Comparativa entre JAX, TensorFlow y PyTorch
- [https://www.computerworld.es/article/2115282/tensorflow-pytorch-y-jax-los-principales-marcos-de-deep-learning.html](https://www.computerworld.es/article/2115282/tensorflow-pytorch-y-jax-los-principales-marcos-de-deep-learning.html)

## Recopilar y estructurar la informaci√≥n
- [https://chatgpt.com/](https://chatgpt.com/)

# üìÉ Licencia
Este proyecto est√° bajo la licencia MIT. Consulta el archivo [`LICENSE`](https://github.com/DavidMoCe/what_is_jax/blob/main/LICENSE.TXT) para m√°s detalles.

