# Â¿QuÃ© es JAX?
**JAX** es una nueva biblioteca de Python de aprendizaje automÃ¡tico de Google, diseÃ±ada para la computaciÃ³n numÃ©rica de alto rendimiento, especialmente en aprendizaje automÃ¡tico y optimizaciÃ³n. Es una alternativa mejorada a NumPy aunque su API para funciones numÃ©ricas se basa en esta.

La ventaja de JAX es que fue concebido para ambos modos de ejecuciÃ³n (Eager y Graph) desde el principio y adolece de los problemas de sus predecesores _PyTorch_ y _Tensorflow_. 

JAX es ampliamente utilizado en modelos de redes neuronales, como en bibliotecas construidas sobre Ã©l, como [**Flax**](https://flax.readthedocs.io/en/latest/) y [**Haiku**](https://dm-haiku.readthedocs.io/en/latest/). 

>[!NOTE]
>ğŸ’¡
>JAX quizÃ¡s sea actualmente lo mÃ¡s avanzado en tÃ©rminos `Machine Learning` (ML) y promete hacer que la programaciÃ³n de aprendizaje automÃ¡tico sea mÃ¡s intuitiva, estructurada y limpia. Y, sobre todo, puede reemplazar con importantes ventajas a **Tensorflow** y **PyTorch**.


# CaracterÃ­sticas
1. **DiferenciaciÃ³n automÃ¡tica**: Permite calcular derivadas de funciones de manera eficiente con `jax.grad()`, Ãºtil en optimizaciÃ³n y aprendizaje profundo. ğŸ”§
2. **CompilaciÃ³n Just-In-Time (JIT)**: Usa `jax.jit()` para acelerar el cÃ³digo al compilarlo con **XLA** (Accelerated Linear Algebra). âš¡
3. **EjecuciÃ³n en GPU/TPU**: Puede ejecutar cÃ³digo en CPU, GPU y TPU sin necesidad de cambios. ğŸ’»ğŸ–¥ï¸
4. **VectorizaciÃ³n automÃ¡tica**: Con `jax.vmap()`, permite aplicar operaciones a mÃºltiples datos en paralelo. ğŸ”„
5. **Operaciones similares a NumPy**: Proporciona una API muy parecida a NumPy (`jax.numpy`), facilitando la transiciÃ³n. â¡ï¸


# ComparaciÃ³n: JAX vs TensorFlow vs PyTorch

JAX, TensorFlow y PyTorch son bibliotecas populares para cÃ³mputo numÃ©rico y aprendizaje profundo. Esta comparaciÃ³n destaca sus diferencias clave en rendimiento, facilidad de uso y aplicaciÃ³n.

## Tabla Comparativa

| **CaracterÃ­stica**       | **JAX** ğŸ¦  | **TensorFlow** ğŸ”µ | **PyTorch** ğŸ”¥ |
|-------------------------|------------|----------------|----------------|
| **Paradigma** | Funcional, sin estado | Declarativo (Graphs) | Imperativo (Define-by-Run) |
| **DiferenciaciÃ³n AutomÃ¡tica** | âœ… `jax.grad()` | âœ… `tf.GradientTape()` | âœ… `torch.autograd` |
| **CompilaciÃ³n JIT (Just-In-Time)** | âœ… `jax.jit()` (XLA) | âœ… `tf.function()` (XLA) | âŒ (Solo en TorchScript) |
| **Ejecuta en GPU/TPU** | âœ… AutomÃ¡tico | âœ… `tf.device()` | âœ… `cuda()` |
| **VectorizaciÃ³n AutomÃ¡tica** | âœ… `jax.vmap()` | âŒ No nativo | âŒ No nativo |
| **Uso de NumPy** | âœ… `jax.numpy` | âŒ No directo | âœ… ConversiÃ³n fÃ¡cil |
| **Ecosistema y Modelos Preentrenados** | â›°ï¸ Creciendo (Flax, Haiku) | ğŸš€ Amplio (TF Hub, Keras) | ğŸ”¥ Extenso (Torch Hub) |
| **Curva de Aprendizaje** | ğŸŸ  Media | ğŸ”´ Alta | ğŸŸ¢ Baja |

# ğŸ¯ ElecciÃ³n de la Mejor OpciÃ³n
- **JAX**: Ideal para **optimizaciones numÃ©ricas, diferenciaciÃ³n avanzada y computaciÃ³n en GPU/TPU**. Usado en investigaciÃ³n. ğŸŒ
- **TensorFlow**: Mejor para **despliegue en producciÃ³n y aplicaciones escalables** en la nube. â˜ï¸
- **PyTorch**: Perfecto para **prototipado rÃ¡pido, investigaciÃ³n y facilidad de uso**. âš¡


# ğŸŒEcosistema
## ğŸ“Œ LibrerÃ­as Implementadas sobre JAX

### **Frameworks de Deep Learning**
- **[Flax](https://github.com/google/flax)** ğŸ—ï¸ â€“ Modular, similar a PyTorch Lightning.
- **[Haiku](https://github.com/deepmind/dm-haiku)** ğŸ”ï¸ â€“ De DeepMind, estructura basada en scopes.
- **[Objax](https://github.com/google/objax)** ğŸ¯ â€“ Enfoque orientado a objetos para ML en producciÃ³n.
- **[Equinox](https://github.com/patrick-kidger/equinox)** ğŸŒ± â€“ Modelos funcionales, sin scopes.

### **OptimizaciÃ³n y MatemÃ¡ticas**
- **[Optax](https://github.com/deepmind/optax)** ğŸ› ï¸ â€“ OptimizaciÃ³n avanzada compatible con Flax y Haiku.
- **[Chex](https://github.com/deepmind/chex)** âœ… â€“ Herramientas de depuraciÃ³n y testing.

### **Aprendizaje por Refuerzo y Redes GrÃ¡ficas**
- **[RLax](https://github.com/deepmind/rlax)** ğŸ† â€“ Algoritmos de RL compatibles con JAX.
- **[Jraph](https://github.com/deepmind/jraph)** ğŸ”— â€“ Redes neuronales grÃ¡ficas (GNNs) en JAX.

### **Integraciones y Herramientas Adicionales**
- **XLA** ğŸš€ â€“ CompilaciÃ³n Just-In-Time para acelerar ejecuciÃ³n en GPU/TPU.
- **`functools.partial()`** ğŸ› ï¸ â€“ Modularidad y funcionalidad avanzada.

## ğŸ’¡ Â¿CuÃ¡l Elegir?
- **Para Deep Learning** â†’ Flax o Haiku. ğŸ§ 
- **Para OptimizaciÃ³n Avanzada** â†’ Optax. ğŸ”§
- **Para Aprendizaje por Refuerzo** â†’ RLax. ğŸ…
- **Para Redes Neuronales GrÃ¡ficas** â†’ Jraph. ğŸ§¬
- **Para DepuraciÃ³n y Pruebas** â†’ Chex. ğŸ› ï¸


# Ejemplo
## OptimizaciÃ³n simple con JAX
#### El cÃ³digo del ejemplo esta en el archivo [`ejemplo.py`](https://github.com/DavidMoCe/what_is_jax/blob/main/optimizaci%C3%B3n_simple_con_jax.py)
Vamos a calcular el mÃ­nimo de una funciÃ³n simple, como la **funciÃ³n cuadrÃ¡tica** \( f(x) = x^2 + 3x + 2 \).

1. **Definir la funciÃ³n**: Creamos la funciÃ³n cuya derivada queremos calcular. ğŸ“
2. **DiferenciaciÃ³n AutomÃ¡tica**: Usamos `jax.grad()` para calcular la derivada de la funciÃ³n. ğŸ“
3. **OptimizaciÃ³n**: Usamos la derivada para hacer una optimizaciÃ³n simple (descenso por gradiente). â¬‡ï¸

## ExplicaciÃ³n:
- **DefiniciÃ³n de la funciÃ³n**:  
  \( f(x) = x^2 + 3x + 2 \) es una funciÃ³n cuadrÃ¡tica simple.

- **CÃ¡lculo de la derivada**:  
  `jax.grad(func)` nos da la derivada de la funciÃ³n, es decir,  
  \( f'(x) = 2x + 3 \).

- **OptimizaciÃ³n**:  
  Usamos **descenso por gradiente**: tomamos el valor de la derivada y lo usamos para ajustar el valor de \( x \) en cada iteraciÃ³n, con el fin de minimizar la funciÃ³n.

## Resultado esperado:

La salida te mostrarÃ¡ cÃ³mo el valor de \( x \) cambia en cada iteraciÃ³n hasta converger al mÃ­nimo de la funciÃ³n. El mÃ­nimo de la funciÃ³n \( f(x) \) es \( x = -1.5 \).


# BibliografÃ­a
## InformaciÃ³n sobre JAX
- [https://eiposgrados.com/blog-python/jax-machine-learning/](https://eiposgrados.com/blog-python/jax-machine-learning/)
- [https://es.eitca.org/inteligencia-artificial/eitc-ai-gcml-google-nube-aprendizaje-autom%C3%A1tico/plataforma-google-cloud-ai/introducci%C3%B3n-a-jax/revisi%C3%B3n-de-examen-introducci%C3%B3n-a-jax/%C2%BFCu%C3%A1les-son-las-caracter%C3%ADsticas-de-jax-que-permiten-el-m%C3%A1ximo-rendimiento-en-el-entorno-de-python%3F/](https://es.eitca.org/inteligencia-artificial/eitc-ai-gcml-google-nube-aprendizaje-autom%C3%A1tico/plataforma-google-cloud-ai/introducci%C3%B3n-a-jax/revisi%C3%B3n-de-examen-introducci%C3%B3n-a-jax/%C2%BFCu%C3%A1les-son-las-caracter%C3%ADsticas-de-jax-que-permiten-el-m%C3%A1ximo-rendimiento-en-el-entorno-de-python%3F/)

## TensorFlow y PyTorch
- [https://www.tensorflow.org/?hl=es-419](https://www.tensorflow.org/?hl=es-419)
- [https://pytorch.org/](https://pytorch.org/)

## Flax y Haiku
- [https://flax.readthedocs.io/en/latest/](https://flax.readthedocs.io/en/latest/)
- [https://dm-haiku.readthedocs.io/en/latest/](https://dm-haiku.readthedocs.io/en/latest/)

## Comparativa entre JAX, TensorFlow y PyTorch
- [https://www.computerworld.es/article/2115282/tensorflow-pytorch-y-jax-los-principales-marcos-de-deep-learning.html](https://www.computerworld.es/article/2115282/tensorflow-pytorch-y-jax-los-principales-marcos-de-deep-learning.html)

## Recopilar y estructurar la informaciÃ³n
- [https://chatgpt.com/](https://chatgpt.com/)
