# -*- coding: utf-8 -*-
"""Optimización simple con JAX.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ri3hA5cxI1CQKI0MzN6e-Vani63IHcYh
"""

# Importamos la librería principal de JAX y la versión de NumPy en JAX
import jax
import jax.numpy as jnp

# Definimos la función
def func(x):
    return x**2 + 3*x + 2

# Calculamos la derivada de la función
grad_func = jax.grad(func)

# Inicializamos el valor de x
x_init = 5.0

# Número de iteraciones para la optimización
learning_rate = 0.1 # Ajuste de de parámetros en cada paso durante la optimización
num_iterations = 20

# Optimización usando descenso por gradiente
x = x_init
for i in range(num_iterations):
    grad = grad_func(x)  # Calculamos la derivada
    x = x - learning_rate * grad  # Actualizamos x según la derivada

    print(f"Iteración {i+1}: x = {x}, f(x) = {func(x)}")

# Resultado
print(f"\nMínimo encontrado en x = {x}, con f(x) = {func(x)}")
# Se acerca a -1.5 que es el mínimo de la función por lo que es correcto, mientras
# más iteracciones, más cercano será el resultado a -1.5, , lo que indica
# que el proceso de optimización está funcionando correctamente.