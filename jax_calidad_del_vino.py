# -*- coding: utf-8 -*-
"""JAX_calidad_del_vino_clase_22_enero_2025.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fa0g11p48Xy1eu7tmRJWOd60meVR_0OI

# JAX. Evaluación de la calidad del vino.

<img width="400px" src="https://drive.google.com/uc?id=1OXOMpdzaU-4kFSUxEyvtH3oLt4pnHA7X">

Vamos a crear y entrenar una red neuronal con JAX para clasificar la calidad del vino utilizando el conjunto de datos `wine`de `sklearn`.

## Importación de librerías
"""

import jax.numpy as jnp # Numpy de JAX
import jax
from jax import random, grad, jit # Jit para acelerar las operaciones
import optax # Para la optimización

import matplotlib.pyplot as plt

# Módulo de scikit-learn para cargar el conjunto de datos del vino
from sklearn.datasets import load_wine

# Función para dividir los datos de entrenamiento y prueba
from sklearn.model_selection import train_test_split

# Clase para normalizar los datos (media 0, desviación 1)
from sklearn.preprocessing import StandardScaler

# Función para calcular la presición
from sklearn.metrics import accuracy_score

# Para mostrar la matriz de confusión
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

"""# Carga y exploración de datos

Usaremos el dataset `wine` de `sklearn`. Este dataset contiene características químicas y organolépticas de vinos y su calidad clasificada en 3 categorías. Haremos un clasificador que diga si un vino es malo, normal o bueno.
"""

# carga de datos
dataset = load_wine()

dataset

# Descripción del dataset
print(dataset.DESCR)

# Datos de cada muestra de vino
dataset.feature_names

# Matriz de características
X = dataset.data
X

X.shape

y = dataset.target

y

# Categorías únicas del target
jnp.unique(y)

# Cuenta cuántos casos hay en cada categoría
categorias, cantidades = jnp.unique(y, return_counts=True)

categorias, cantidades

# Guarda en un diccionario cuántas ocurrencias hay para cada categoría
distribucion = dict(zip(categorias.tolist(), cantidades.tolist()))

# Otra manera de hacerlo
from collections import Counter
distribucion = dict(Counter(y))

# Otra más
distribucion = dict(zip(tuple(jnp.unique(y).tolist()), jnp.bincount(y).tolist()))

distribucion

# Gráfica de la distribución
color_vino = '#8B0000'
plt.bar(distribucion.keys(), distribucion.values(), color=color_vino)
plt.xticks(ticks=list(distribucion.keys()), labels=["Malo", "Normal", "Bueno"])
plt.xlabel('Categoría')
plt.ylabel('Cantidad')
plt.title('Distribución de las categorías del vino')
plt.show()

"""# División y escalado de datos

Dividimos los datos en conjunto de entrenamiento y prueba.

Luego, normalizamos las características para que cada columna tenga una media 0 y desviación estándar 1. Esto suele mejorar el rendimiento del modelo, sobre todo con redes neuronales que son muy sensibles a las diferencias de escala.

Supongamos que una característica tiene los valores originales:

$[10, 12, 14, 16, 18]$

La media es $μ = 14$ y la desviación estándar $σ = \sqrt{\frac{\sum{(x_i - μ)^2}}{N}} = 2.83$

Después de la normalización:

$X_{normalizado} = \frac{X - \mu}{\sigma}$

Los valores se transforman en:

$[-1.41, -0.71, 0.0, 0.71, 1.41]$
"""

# Divide los datos en entrenamiento y prueba
# random_state=42 hace que la división sea reproducible
# parecido a cuando se inicializa una secuencia de números aleatorios con una semilla
# stratify=y garantiza que la proporción de clases en el conjunto de entrenamiento
# y prueba sea similar a la del conjunto original, evitando el sesgo
# shuffle=True baraja/mezcla los datos
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y, shuffle=True)

# Normalización de datos
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

X_train

X_train.dtype

"""# Conversión de arrays a tensores de JAX"""

X_train, X_test = jnp.array(X_train), jnp.array(X_test)
y_train, y_test = jnp.array(y_train), jnp.array(y_test)

"""# Definición del modelo

Definimos una red neuronal con una capa oculta y funciones de activación ReLU. La salida usa una función de activacion Softmax.
En PyTorch se utilizaría `nn.sequential`, pero en Jax se hace de la siguiente forma.
"""

# Inicialización de parámetros
def init_params(key, input_dim, hidden_dim1, hidden_dim2, output_dim):
    # Dividir la clave de aleatorización en tres partes
    key1, key2, key3 = random.split(key, 3)

    # Crear un diccionario con los pesos y sesgos de cada capa
    params = {
        'W1': random.normal(key1, (input_dim, hidden_dim1)) * 0.1, # Pesos de la primera capa
        'b1': jnp.zeros((hidden_dim1,)), # Sesgos de la primera capa
        'W2': random.normal(key2, (hidden_dim1, hidden_dim2)) * 0.1, # Pesos de la segunda capa
        'b2': jnp.zeros((hidden_dim2,)), # Sesgos de la segunda capa
        'W3': random.normal(key3, (hidden_dim2, output_dim)) * 0.1, # Pesos de la tercera capa
        'b3': jnp.zeros((output_dim,)) # Sesgos de la tercera capa
    }
    return params

# Generar una clave aleatoria para la inicialización
key = random.PRNGKey(0)
params = init_params(key, X_train.shape[1], 16, 8, len(jnp.unique(y)))

# Definición del modelo
def forward(params, x):
    x = jnp.dot(x, params['W1']) + params['b1'] # Capa de entrada
    x = jax.nn.relu(x) # Activación ReLU
    x = jnp.dot(x, params['W2']) + params['b2'] # Capa oculta
    x = jax.nn.relu(x) # Activación ReLU
    x = jnp.dot(x, params['W3']) + params['b3'] # Capa de salida
    return jax.nn.softmax(x, axis=1) # Activación Softmax

# Función de pérdida, calcula la entropía cruzada entre las predicciones y las etiquetas reales.
def loss_fn(params, x, y):
    logits = forward(params, x)
    y_one_hot = jax.nn.one_hot(y, num_classes=logits.shape[1])
    return -jnp.mean(jnp.sum(y_one_hot * jnp.log(logits + 1e-8), axis=1))

# Optimizador Adam con tasa de aprendizaje de 0.001
# El optimizador se encarga de ajustar los pesos y sesgos en cada iteracción del entrenamiento
learning_rate = 0.001
optimizer = optax.adam(learning_rate)
opt_state = optimizer.init(params)

"""## Entrenamiento del modelo

Entrenaremos el modelo durante 100 épocas, imprimiendo la pérdida cada 10 épocas.
"""

# Entrenamiento
def update(params, opt_state, x, y):
    grads = grad(loss_fn)(params, x, y)
    updates, opt_state = optimizer.update(grads, opt_state)
    params = optax.apply_updates(params, updates)
    return params, opt_state

epochs = 200
for epoch in range(epochs):
    params, opt_state = update(params, opt_state, X_train, y_train)
    if (epoch + 1) % 10 == 0:
        loss = loss_fn(params, X_train, y_train)
        print(f"Epoch: [{epoch+1}/{epochs}], Loss: {loss:.4f}")

"""# Evaluación del modelo

Calculamos la presición en el conjunto de prueba àra evaluar el rendimiento.
"""

# Evaluación
y_pred = jnp.argmax(forward(params, X_test), axis=1)
test_accuracy = accuracy_score(y_test, y_pred)
print(f"Presición en el conjunto de pruebas: {(test_accuracy * 100):.2f}%")

# Calcula la matriz de confusión
conf_matrix = confusion_matrix(y_test, y_pred)
conf_matrix

# Visualiza la matriz de confusión
disp = ConfusionMatrixDisplay(conf_matrix, display_labels=dataset.target_names)
disp.plot(cmap='Blues')
plt.title('Matriz de confusión')
plt.show()